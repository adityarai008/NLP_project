{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10zm1RJFFB7nWYrL37nL2CuA0u5YwYFXt",
      "authorship_tag": "ABX9TyPVR/KEjmCAJXc5e562eL2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityarai008/NLP_project/blob/main/B_Tech_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "aUoKplTpfrGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk indic-nlp-library\n"
      ],
      "metadata": {
        "id": "jDSBkl-JZs9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "FnXHnF9WK-T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "_KqSQ-5HUNdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from spacy.lang.en import English\n",
        "from spacy.pipeline import EntityRuler\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from html.parser import HTMLParser\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import html\n",
        "import torch\n",
        "nltk.download('indian')\n",
        "nltk.data.path.append('/root/nltk_data/corpora/stopwords/hindi')"
      ],
      "metadata": {
        "id": "REafSh3vK1yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "id": "z-SFB0axcZEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab = set(nltk.corpus.words.words())\n"
      ],
      "metadata": {
        "id": "TtpQ1PUTccvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = html.unescape(text)\n",
        "    text = re.sub(r'http\\S+|@\\S+|#\\S+', '', text)\n",
        "    text = \"\".join([i.lower() for i in text if i not in string.punctuation])\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stopwords_eng = stopwords.words('english')\n",
        "    filtered_tokens = [word for word in word_tokens if word not in stopwords_eng]\n",
        "    cleaned_text = \" \".join(filtered_tokens)\n",
        "    return cleaned_text\n"
      ],
      "metadata": {
        "id": "fUFf5V1wKhLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = glob.glob(\"./drive/MyDrive/Hindi-english/*.txt\")\n",
        "cleaned_texts = []\n",
        "for file in files:\n",
        "        with open(file, \"r\", encoding=\"utf-8\") as input_file:\n",
        "            text = input_file.read()\n",
        "        \n",
        "        cleaned_text = clean_text(text)\n",
        "\n",
        "        cleaned_texts.append(cleaned_text)\n",
        "    \n"
      ],
      "metadata": {
        "id": "gKi68WeyKqTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_texts)"
      ],
      "metadata": {
        "id": "LDssc58t0yM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"salesken/translation-hi-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"salesken/translation-hi-en\")"
      ],
      "metadata": {
        "id": "1ivsqF6CRt3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text):\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
        "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return translated_text\n"
      ],
      "metadata": {
        "id": "YpAkT422pVtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "translated_texts = []\n",
        "for text in cleaned_texts:\n",
        "    translated_text = translate_text(text)\n",
        "    \n",
        "    print(translated_text)\n",
        "    translated_texts.append(translated_text)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "hKhLWihF9UFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translated_texts)"
      ],
      "metadata": {
        "id": "p_TN5Ry3v98_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "1FKKeowia5X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
        "    return lemmatized_text, entities\n",
        "\n",
        "texts = []\n",
        "entities_list = []\n",
        "for text in translated_texts:\n",
        "    processed_text, entities = preprocess_text(text)\n",
        "    texts.append(processed_text)\n",
        "    entities_list.extend(entities)\n",
        "print(texts)\n",
        "print(entities_list)\n"
      ],
      "metadata": {
        "id": "P-8GEAt7Md6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "k = 5  # Choose the appropriate number of clusters for your data\n",
        "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=5, verbose=1)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "clusters = {}\n",
        "for i, label in enumerate(labels):\n",
        "    if label not in clusters:\n",
        "        clusters[label] = []\n",
        "    clusters[label].append(texts[i])\n",
        "print(clusters)\n"
      ],
      "metadata": {
        "id": "7HY-LNpwMhhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n"
      ],
      "metadata": {
        "id": "ZWN12LY6LEpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q4eFFOnaxY2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def summarize(text, entities):\n",
        "    # Include important entities in the summarization input\n",
        "    text_with_entities = f\"{', '.join(entities)} - {text}\"\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text_with_entities, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0])\n",
        "    return summary\n",
        "\n",
        "cluster_summaries = {}\n",
        "# Generate summaries for each cluster\n",
        "cluster_summaries = {}\n",
        "for cluster_id, cluster_texts in clusters.items():\n",
        "    combined_text = \"\"\n",
        "    combined_entities = []\n",
        "    for text in cluster_texts:\n",
        "        processed_text, entities = preprocess_text(text)\n",
        "        combined_text += processed_text + \" \"\n",
        "        combined_entities.extend(entities)\n",
        "    summary = summarize(combined_text, combined_entities)\n",
        "    cluster_summaries[cluster_id] = summary"
      ],
      "metadata": {
        "id": "DUWJH99VORM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text = \"\"\n",
        "for cluster_id, summary in cluster_summaries.items():\n",
        "    print(f\"Cluster {cluster_id} summary:\")\n",
        "    combined_text += (summary)\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "S3-5aHZbLGXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MB35C0Ong6PK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}